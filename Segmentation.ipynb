{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melanoma Segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.layers import Input, concatenate, Conv2DTranspose\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "smooth = 1\n",
    "\n",
    "if not os.path.exists('pickled'):\n",
    "    os.makedirs('pickled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Data Loading\n",
    "data = []\n",
    "labels = []\n",
    "if not os.path.exists('pickled/seg_data.pkl'):\n",
    "    for file in os.listdir('melanoma'):        \n",
    "        image = cv2.imread('melanoma/' + file)\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        image = img_to_array(image)\n",
    "        data.append(image)\n",
    "        \n",
    "        path = str('gt/' + file.split('.')[0] + '_segmentation.png')\n",
    "        \n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        image = img_to_array(image)\n",
    "        labels.append(image)\n",
    "\n",
    "    for file in os.listdir(\"others\"):\n",
    "        image = cv2.imread('others/' + file)\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        image = img_to_array(image)\n",
    "        data.append(image)\n",
    "        \n",
    "        path = str('gt/' + file.split('.')[0] + '_segmentation.png')\n",
    "        \n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        image = img_to_array(image)\n",
    "        labels.append(image)\n",
    "        \n",
    "    d = {}\n",
    "    d['data'] = data\n",
    "    d['labels'] = labels\n",
    "    pickle.dump(d, open('pickled/seg_data.pkl', 'wb'))\n",
    "else:\n",
    "    d = pickle.load(open( 'pickled/seg_data.pkl', 'rb'))\n",
    "    data = d['data']\n",
    "    labels = d['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels, dtype=\"float\") / 255.0\n",
    "\n",
    "(x_train, x_test, y_train, y_test) = train_test_split(data,labels, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINING METRICS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1600, 64, 64, 3)\n",
      "1600 train samples\n",
      "400 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_classes = 2\n",
    "epochs = 10\n",
    "\n",
    "img_rows, img_cols = 64, 64\n",
    "\n",
    "input_shape = (img_rows, img_cols,3)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def down(filters, inputs):\n",
    "    down_ = Conv2D(filters, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    down_res = Conv2D(32, (3, 3), activation='relu', padding='same')(down_)\n",
    "    down_pool = MaxPooling2D(pool_size=(2, 2))(down_)\n",
    "    return down_res, down_pool\n",
    "    \n",
    "def up(filters, inputs, down_):\n",
    "    up_ = concatenate([Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same')(inputs), down_], axis=3)\n",
    "    up_ = Conv2D(256, (3, 3), activation='relu', padding='same')(up_)\n",
    "    up_ = Conv2D(256, (3, 3), activation='relu', padding='same')(up_)\n",
    "    return up_\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "epochs = 10\n",
    "\n",
    "img_rows, img_cols = 64, 64\n",
    "\n",
    "input_shape = (img_rows, img_cols,3)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "inputs = Input((img_rows, img_cols, 3))\n",
    "\n",
    "down0a, down0a_res = down(2, inputs)\n",
    "down0, down0_res = down(4, down0a)\n",
    "down1, down1_res = down(8, down0)\n",
    "down2, down2_res = down(16, down1)\n",
    "down3, down3_res = down(32, down2)\n",
    "\n",
    "\n",
    "center = Conv2D(128, (3, 3), padding='same')(down3)\n",
    "center = BatchNormalization(epsilon=1e-4)(center)\n",
    "center = Activation('relu')(center)\n",
    "\n",
    "up3 = up(32, center, down3_res)\n",
    "up2 = up(16, up3, down2_res)\n",
    "up1 = up(8, up2, down1_res)\n",
    "up0 = up(4, up1, down0_res)\n",
    "up0a = up(2, up0, down0a_res)\n",
    "\n",
    "classify = Conv2D(num_classes, (1, 1), activation='sigmoid', name='final_layer')(up0a)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=classify)\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=[dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[inputs], outputs=[classify])\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=[dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 435s 272ms/step - loss: -0.2746 - dice_coef: 0.2746 - val_loss: -0.2958 - val_dice_coef: 0.2958\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 355s 222ms/step - loss: -0.2758 - dice_coef: 0.2758 - val_loss: -0.2971 - val_dice_coef: 0.2971\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 499s 312ms/step - loss: -0.2771 - dice_coef: 0.2771 - val_loss: -0.2989 - val_dice_coef: 0.2989\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 356s 222ms/step - loss: -0.2796 - dice_coef: 0.2796 - val_loss: -0.3044 - val_dice_coef: 0.3044\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 359s 224ms/step - loss: -0.3076 - dice_coef: 0.3076 - val_loss: -0.4125 - val_dice_coef: 0.4125\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 365s 228ms/step - loss: -0.5028 - dice_coef: 0.5028 - val_loss: -0.6003 - val_dice_coef: 0.6003\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 372s 233ms/step - loss: -0.5888 - dice_coef: 0.5888 - val_loss: -0.5721 - val_dice_coef: 0.5721\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 362s 226ms/step - loss: -0.5936 - dice_coef: 0.5936 - val_loss: -0.6111 - val_dice_coef: 0.6111\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 378s 237ms/step - loss: -0.6091 - dice_coef: 0.6091 - val_loss: -0.6232 - val_dice_coef: 0.6232\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 380s 237ms/step - loss: -0.6242 - dice_coef: 0.6242 - val_loss: -0.6477 - val_dice_coef: 0.6477\n",
      "Score: [-0.64772118091583253, 0.6477211904525757]\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
